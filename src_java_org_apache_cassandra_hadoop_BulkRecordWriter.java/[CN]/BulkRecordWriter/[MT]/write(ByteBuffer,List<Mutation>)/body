{
  setTypes(value.get(0));
  prepareWriter();
  SSTableSimpleUnsortedWriter ssWriter=(SSTableSimpleUnsortedWriter)writer;
  ssWriter.newRow(keybuff);
  for (  Mutation mut : value) {
    if (cfType == CFType.SUPER) {
      ssWriter.newSuperColumn(mut.getColumn_or_supercolumn().getSuper_column().name);
      if (colType == ColType.COUNTER)       for (      CounterColumn column : mut.getColumn_or_supercolumn().getCounter_super_column().columns)       ssWriter.addCounterColumn(column.name,column.value);
 else {
        for (        Column column : mut.getColumn_or_supercolumn().getSuper_column().columns) {
          if (column.ttl == 0)           ssWriter.addColumn(column.name,column.value,column.timestamp);
 else           ssWriter.addExpiringColumn(column.name,column.value,column.timestamp,column.ttl,System.currentTimeMillis() + ((long)column.ttl * 1000));
        }
      }
    }
 else {
      if (colType == ColType.COUNTER)       ssWriter.addCounterColumn(mut.getColumn_or_supercolumn().counter_column.name,mut.getColumn_or_supercolumn().counter_column.value);
 else {
        if (mut.getColumn_or_supercolumn().column.ttl == 0)         ssWriter.addColumn(mut.getColumn_or_supercolumn().column.name,mut.getColumn_or_supercolumn().column.value,mut.getColumn_or_supercolumn().column.timestamp);
 else         ssWriter.addExpiringColumn(mut.getColumn_or_supercolumn().column.name,mut.getColumn_or_supercolumn().column.value,mut.getColumn_or_supercolumn().column.timestamp,mut.getColumn_or_supercolumn().column.ttl,System.currentTimeMillis() + ((long)(mut.getColumn_or_supercolumn().column.ttl) * 1000));
      }
    }
    if (null != progress)     progress.progress();
    if (null != context)     HadoopCompat.progress(context);
  }
}
