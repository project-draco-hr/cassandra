{
  if (insertStatement == null) {
synchronized (this) {
      if (insertStatement == null) {
        maybeLoadSchemaInfo(settings);
        Set<ColumnMetadata> keyColumns=com.google.common.collect.Sets.newHashSet(tableMetaData.getPrimaryKey());
        StringBuilder sb=new StringBuilder();
        sb.append("UPDATE \"").append(tableName).append("\" SET ");
        StringBuilder pred=new StringBuilder();
        pred.append(" WHERE ");
        boolean firstCol=true;
        boolean firstPred=true;
        for (        ColumnMetadata c : tableMetaData.getColumns()) {
          if (keyColumns.contains(c)) {
            if (firstPred)             firstPred=false;
 else             pred.append(" AND ");
            pred.append(c.getName()).append(" = ?");
          }
 else {
            if (firstCol)             firstCol=false;
 else             sb.append(",");
            sb.append(c.getName()).append(" = ");
switch (c.getType().getName()) {
case SET:
case LIST:
case COUNTER:
              sb.append(c.getName()).append(" + ?");
            break;
default :
          sb.append("?");
        break;
    }
  }
}
sb.append(pred);
if (insert == null) insert=new HashMap<>();
lowerCase(insert);
partitions=select(settings.insert.batchsize,"partitions","fixed(1)",insert,OptionDistribution.BUILDER);
selectchance=select(settings.insert.selectRatio,"select","fixed(1)/1",insert,OptionRatioDistribution.BUILDER);
batchType=settings.insert.batchType != null ? settings.insert.batchType : !insert.containsKey("batchtype") ? BatchStatement.Type.LOGGED : BatchStatement.Type.valueOf(insert.remove("batchtype"));
if (!insert.isEmpty()) throw new IllegalArgumentException("Unrecognised insert option(s): " + insert);
Distribution visits=settings.insert.visits.get();
double minBatchSize=selectchance.get().min() * partitions.get().minValue() * generator.minRowCount* (1d / visits.maxValue());
double maxBatchSize=selectchance.get().max() * partitions.get().maxValue() * generator.maxRowCount* (1d / visits.minValue());
System.out.printf("Generating batches with [%d..%d] partitions and [%.0f..%.0f] rows (of [%.0f..%.0f] total rows in the partitions)%n",partitions.get().minValue(),partitions.get().maxValue(),minBatchSize,maxBatchSize,partitions.get().minValue() * generator.minRowCount,partitions.get().maxValue() * generator.maxRowCount);
if (generator.maxRowCount > 100 * 1000 * 1000) System.err.printf("WARNING: You have defined a schema that permits very large partitions (%.0f max rows (>100M))%n",generator.maxRowCount);
if (batchType == BatchStatement.Type.LOGGED && maxBatchSize > 65535) {
  System.err.printf("ERROR: You have defined a workload that generates batches with more than 65k rows (%.0f), but have required the use of LOGGED batches. There is a 65k row limit on a single batch.%n",selectchance.get().max() * partitions.get().maxValue() * generator.maxRowCount);
  System.exit(1);
}
if (maxBatchSize > 100000) System.err.printf("WARNING: You have defined a schema that permits very large batches (%.0f max rows (>100K)). This may OOM this stress client, or the server.%n",selectchance.get().max() * partitions.get().maxValue() * generator.maxRowCount);
JavaDriverClient client=settings.getJavaDriverClient();
String query=sb.toString();
try {
  thriftInsertId=settings.getThriftClient().prepare_cql3_query(query,Compression.NONE);
}
 catch (TException e) {
  throw new RuntimeException(e);
}
insertStatement=client.prepare(query);
}
}
}
return new SchemaInsert(timer,generator,settings,partitions.get(),selectchance.get(),thriftInsertId,insertStatement,ThriftConversion.fromThrift(settings.command.consistencyLevel),batchType);
}
