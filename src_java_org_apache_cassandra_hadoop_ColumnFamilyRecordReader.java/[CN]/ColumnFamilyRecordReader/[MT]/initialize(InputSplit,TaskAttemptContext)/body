{
  this.split=(ColumnFamilySplit)split;
  Configuration conf=HadoopCompat.getConfiguration(context);
  KeyRange jobRange=ConfigHelper.getInputKeyRange(conf);
  filter=jobRange == null ? null : jobRange.row_filter;
  predicate=ConfigHelper.getInputSlicePredicate(conf);
  boolean widerows=ConfigHelper.getInputIsWide(conf);
  isEmptyPredicate=isEmptyPredicate(predicate);
  totalRowCount=(this.split.getLength() < Long.MAX_VALUE) ? (int)this.split.getLength() : ConfigHelper.getInputSplitSize(conf);
  batchSize=ConfigHelper.getRangeBatchSize(conf);
  cfName=ConfigHelper.getInputColumnFamily(conf);
  consistencyLevel=ConsistencyLevel.valueOf(ConfigHelper.getReadConsistencyLevel(conf));
  keyspace=ConfigHelper.getInputKeyspace(conf);
  if (batchSize < 2)   throw new IllegalArgumentException("Minimum batchSize is 2.  Suggested batchSize is 100 or more");
  try {
    if (client != null)     return;
    String location=getLocation();
    int port=ConfigHelper.getInputRpcPort(conf);
    client=ColumnFamilyInputFormat.createAuthenticatedClient(location,port,conf);
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
  iter=widerows ? new WideRowIterator() : new StaticRowIterator();
  logger.debug("created {}",iter);
}
