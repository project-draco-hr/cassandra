{
  logger_.info("Compacting [" + StringUtils.join(files,",") + "]");
  String compactionFileLocation=DatabaseDescriptor.getDataFileLocationForTable(table_,getExpectedCompactedFileSize(files));
  if (compactionFileLocation == null) {
    String maxFile=getMaxSizeFile(files);
    files.remove(maxFile);
    return doFileCompaction(files,minBufferSize);
  }
  String newfile=null;
  long startTime=System.currentTimeMillis();
  long totalBytesRead=0;
  long totalBytesWritten=0;
  long totalkeysRead=0;
  long totalkeysWritten=0;
  PriorityQueue<FileStruct> pq=initializePriorityQueue(files,null);
  if (pq.isEmpty()) {
    logger_.warn("Nothing to compact (all files empty or corrupt)");
    return 0;
  }
  String mergedFileName=getTempFileName(files);
  SSTableWriter writer=null;
  SSTableReader ssTable=null;
  String lastkey=null;
  List<FileStruct> lfs=new ArrayList<FileStruct>();
  DataOutputBuffer bufOut=new DataOutputBuffer();
  int expectedBloomFilterSize=SSTableReader.getApproximateKeyCount(files);
  expectedBloomFilterSize=(expectedBloomFilterSize > 0) ? expectedBloomFilterSize : SSTableReader.indexInterval();
  if (logger_.isDebugEnabled())   logger_.debug("Expected bloom filter size : " + expectedBloomFilterSize);
  List<ColumnFamily> columnFamilies=new ArrayList<ColumnFamily>();
  while (pq.size() > 0 || lfs.size() > 0) {
    FileStruct fs=null;
    if (pq.size() > 0) {
      fs=pq.poll();
    }
    if (fs != null && (lastkey == null || lastkey.equals(fs.getKey()))) {
      lastkey=fs.getKey();
      lfs.add(fs);
    }
 else {
      Collections.sort(lfs,new FileStructComparator());
      ColumnFamily columnFamily;
      bufOut.reset();
      if (lfs.size() > 1) {
        for (        FileStruct filestruct : lfs) {
          if (columnFamilies.size() > 1) {
            merge(columnFamilies);
          }
          columnFamilies.add(filestruct.getColumnFamily());
        }
        columnFamily=resolveAndRemoveDeleted(columnFamilies);
        columnFamilies.clear();
        if (columnFamily != null) {
          ColumnFamily.serializerWithIndexes().serialize(columnFamily,bufOut);
        }
      }
 else {
        FileStruct filestruct=lfs.get(0);
        ColumnFamily.serializerWithIndexes().serialize(filestruct.getColumnFamily(),bufOut);
      }
      if (writer == null) {
        String fname=new File(compactionFileLocation,mergedFileName).getAbsolutePath();
        writer=new SSTableWriter(fname,expectedBloomFilterSize,StorageService.getPartitioner());
      }
      writer.append(lastkey,bufOut);
      totalkeysWritten++;
      for (      FileStruct filestruct : lfs) {
        filestruct.advance(true);
        if (filestruct.isExhausted()) {
          continue;
        }
        pq.add(filestruct);
        totalkeysRead++;
      }
      lfs.clear();
      lastkey=null;
      if (fs != null) {
        pq.add(fs);
      }
    }
  }
  if (writer != null) {
    ssTable=writer.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_));
    newfile=writer.getFilename();
  }
  sstableLock_.writeLock().lock();
  try {
    for (    String file : files) {
      ssTables_.remove(file);
    }
    if (newfile != null) {
      ssTables_.put(newfile,ssTable);
      totalBytesWritten+=(new File(newfile)).length();
    }
    for (    String file : files) {
      SSTableReader.get(file).delete();
    }
  }
  finally {
    sstableLock_.writeLock().unlock();
  }
  String format="Compacted to %s.  %d/%d bytes for %d/%d keys read/written.  Time: %dms.";
  long dTime=System.currentTimeMillis() - startTime;
  logger_.info(String.format(format,newfile,totalBytesRead,totalBytesWritten,totalkeysRead,totalkeysWritten,dTime));
  return files.size();
}
